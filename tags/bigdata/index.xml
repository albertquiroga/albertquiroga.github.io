<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>bigdata on Elaborative Encoding</title><link>https://albertquiroga.github.io/tags/bigdata/</link><description>Recent content in bigdata on Elaborative Encoding</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Albert Quiroga</copyright><lastBuildDate>Mon, 03 May 2021 19:00:00 +0100</lastBuildDate><atom:link href="https://albertquiroga.github.io/tags/bigdata/index.xml" rel="self" type="application/rss+xml"/><item><title>The "Spark writes too many output files" conundrum</title><link>https://albertquiroga.github.io/posts/spark-output-files-conundrum/</link><pubDate>Mon, 03 May 2021 19:00:00 +0100</pubDate><guid>https://albertquiroga.github.io/posts/spark-output-files-conundrum/</guid><description>As a Big Data engineer, one of the most common problems I see people facing is &amp;ldquo;why is Spark writing 300 files&amp;rdquo;. Many engineers (specially ones in BI environments) are used to having datasets written as a single file, typically in a plain text format like CSV or JSON. Not only that, but many tools like Databases or BI dashboards expect data to be part of a single file - which obviously clases with how Spark works.</description></item></channel></rss>