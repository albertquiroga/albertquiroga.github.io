<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>The "Spark writes too many output files" conundrum | Elaborative Encoding</title><meta name=description content="Albert Quiroga's personal site."><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="The &#34;Spark writes too many output files&#34; conundrum"><meta property="og:description" content="As a Big Data engineer, one of the most common problems I see people facing is &ldquo;why is Spark writing 300 files&rdquo;. Many engineers (specially ones in BI environments) are used to having datasets written as a single file, typically in a plain text format like CSV or JSON. Not only that, but many tools like Databases or BI dashboards expect data to be part of a single file - which obviously clases with how Spark works."><meta property="og:type" content="article"><meta property="og:url" content="https://albertquiroga.github.io/posts/spark-output-files-conundrum/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-05-03T19:00:00+01:00"><meta property="article:modified_time" content="2021-05-03T19:00:00+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="The &#34;Spark writes too many output files&#34; conundrum"><meta name=twitter:description content="As a Big Data engineer, one of the most common problems I see people facing is &ldquo;why is Spark writing 300 files&rdquo;. Many engineers (specially ones in BI environments) are used to having datasets written as a single file, typically in a plain text format like CSV or JSON. Not only that, but many tools like Databases or BI dashboards expect data to be part of a single file - which obviously clases with how Spark works."><link rel=stylesheet href=https://albertquiroga.github.io/css/style-dark.css><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script><script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://albertquiroga.github.io/images/favicon.ico></head><body class="max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-bars fa-lg"></i></a>
<a id=menu-icon-tablet href=#><i class="fas fa-bars fa-lg"></i></a>
<a id=top-icon-tablet href=# onclick="$('html, body').animate({scrollTop:0},'fast')" style=display:none><i class="fas fa-chevron-up fa-lg"></i></a>
<span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=/posts>Writings</a></li><li><a href=/tags>Tags</a></li><li><a href=/about>About</a></li></ul></span><br><span id=actions><ul><li><a class=icon href=https://albertquiroga.github.io/posts/controlling-you-windows-10-pc-with-homekit/><i class="fas fa-chevron-left" aria-hidden=true onmouseover="$('#i-prev').toggle()" onmouseout="$('#i-prev').toggle()"></i></a></li><li><a class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast')"><i class="fas fa-chevron-up" aria-hidden=true onmouseover="$('#i-top').toggle()" onmouseout="$('#i-top').toggle()"></i></a></li><li><a class=icon href=#><i class="fas fa-share-alt" aria-hidden=true onmouseover="$('#i-share').toggle()" onmouseout="$('#i-share').toggle()" onclick="return $('#share').toggle(),!1"></i></a></li></ul><span id=i-prev class=info style=display:none>Previous post</span>
<span id=i-next class=info style=display:none>Next post</span>
<span id=i-top class=info style=display:none>Back to top</span>
<span id=i-share class=info style=display:none>Share post</span></span><br><div id=share style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f"><i class="fab fa-facebook" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f&text=The%20%22Spark%20writes%20too%20many%20output%20files%22%20conundrum"><i class="fab fa-twitter" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f&title=The%20%22Spark%20writes%20too%20many%20output%20files%22%20conundrum"><i class="fab fa-linkedin" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f&is_video=false&description=The%20%22Spark%20writes%20too%20many%20output%20files%22%20conundrum"><i class="fab fa-pinterest" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=The%20%22Spark%20writes%20too%20many%20output%20files%22%20conundrum&body=Check out this article: https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f"><i class="fas fa-envelope" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f&title=The%20%22Spark%20writes%20too%20many%20output%20files%22%20conundrum"><i class="fab fa-get-pocket" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f&title=The%20%22Spark%20writes%20too%20many%20output%20files%22%20conundrum"><i class="fab fa-reddit" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f&name=The%20%22Spark%20writes%20too%20many%20output%20files%22%20conundrum&description=As%20a%20Big%20Data%20engineer%2c%20one%20of%20the%20most%20common%20problems%20I%20see%20people%20facing%20is%20%26ldquo%3bwhy%20is%20Spark%20writing%20300%20files%26rdquo%3b.%20Many%20engineers%20%28specially%20ones%20in%20BI%20environments%29%20are%20used%20to%20having%20datasets%20written%20as%20a%20single%20file%2c%20typically%20in%20a%20plain%20text%20format%20like%20CSV%20or%20JSON.%20Not%20only%20that%2c%20but%20many%20tools%20like%20Databases%20or%20BI%20dashboards%20expect%20data%20to%20be%20part%20of%20a%20single%20file%20-%20which%20obviously%20clases%20with%20how%20Spark%20works."><i class="fab fa-tumblr" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f&t=The%20%22Spark%20writes%20too%20many%20output%20files%22%20conundrum"><i class="fab fa-hacker-news" aria-hidden=true></i></a></li></ul></div><div id=toc><nav id=TableOfContents><ul><li><a href=#why-does-this-happen>Why does this happen?</a></li><li><a href=#how-do-i-fix-this>How do I fix this?</a><ul><li><a href=#repartitioning-to-1-partition>Repartitioning to 1 partition</a></li><li><a href=#compacting-after-writing>Compacting after writing</a></li><li><a href=#the-s3-workaround>The S3 workaround</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">The "Spark writes too many output files" conundrum</h1><div class=meta><div class=postdate><time datetime="2021-05-03 19:00:00 +0100 +0100" itemprop=datePublished>2021-05-03</time></div><div class=article-tag><i class="fas fa-tag"></i>
<a class=tag-link href=/tags/spark rel=tag>spark</a>
,
<a class=tag-link href=/tags/glue rel=tag>glue</a>
,
<a class=tag-link href=/tags/bigdata rel=tag>bigdata</a>
,
<a class=tag-link href=/tags/s3 rel=tag>s3</a></div></div></header><div class=content itemprop=articleBody><p>As a Big Data engineer, one of the most common problems I see people facing is &ldquo;why is Spark writing 300 files&rdquo;. Many engineers (specially ones in BI environments) are used to having datasets written as a single file, typically in a plain text format like CSV or JSON. Not only that, but many tools like Databases or BI dashboards expect data to be part of a single file - which obviously clases with how Spark works.</p><h2 id=why-does-this-happen>Why does this happen?</h2><p><em>Warning: the text below is a simplified, high-level explanation of Spark internals. If you want deeper understanding, please refer to <a href=https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/>the bible</a>.</em></p><p>Spark is a Big Data framework designed to process VERY large amounts of data. The way it achieves this is by distributing your data over a cluster of nodes, and having each node process it independently rather than a single machine do all the work. This distribution (or partitioning) of data is the culprit here.</p><p>Whenever you execute a Spark application, Spark will check the contents of the source path you specified. Take an AWS S3 path for example:</p><ol><li>Spark will list how many files are in the path along with their size, then create in-memory partitions out of those files.</li><li>Each partition will be mapped to a Spark task, and then Spark will launch as many Spark executors as necessary to process these tasks concurrently.</li><li>Each executor will execute your job&rsquo;s transformations on a different node in a cluster</li><li>Once all operations in your job are completed, each executor will then write each in-memory partition as an output file to your destination path.</li></ol><p>This is why many output files are generated: there will be one output file for each Spark partition your dataset is partitioned into at the moment of writing.</p><h2 id=how-do-i-fix-this>How do I fix this?</h2><p>Unfortunately, there&rsquo;s no easy solution here. Following the aforementioned logic, the only solution would be to have only one Spark partition before writing - but let me explain why that&rsquo;s a terrible solution.</p><h3 id=repartitioning-to-1-partition>Repartitioning to 1 partition</h3><p>As explained above, Spark partitions your data in order to have many nodes process it instead of a single one. This lets you do things like <a href=https://opensource.com/business/15/1/apache-spark-new-world-record>processing 100 TB of data in-memory</a> by having many small machines instead of having to somehow build a single machine with more than 100 TB of RAM.</p><p>Even though Spark and its connectors have their own logic to handle partitioning for you, you can still manually alter this with either <a href=https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html#repartition-org.apache.spark.sql.Column...->repartition</a> or <a href=https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html#coalesce-int->coalesce</a>. So you could use coalesce (<a href=https://stackoverflow.com/questions/31610971/spark-repartition-vs-coalesce>and only coalesce</a>) to reduce the number of partitions to 1, then write.</p><p>Now I think you&rsquo;ll probably know where I&rsquo;m heading with this. If you really are using Spark to process large amounts of data, changing the number of partitions to 1 will:</p><ul><li>Send all data to one node, which means a massive network shuffle. Bad performance.</li><li>Have only one thread of one executor process all your data. Again, bad performance, but also you are most likely going to crash the executor out of memory.</li></ul><p>The truth of the matter is Spark was never designed to do this, and even if it gave you the option, many distributed systems like S3 or HDFS favor distributed reads rather than having thousands of API calls or read operations towards the same file.</p><p><strong>TL;DR: only do this when strictly necessary</strong>, and be aware that it is a terrible practice.</p><h3 id=compacting-after-writing>Compacting after writing</h3><p>If writing to a single file is not possible, the only solution is to compact files after writing them. How this happens will greatly depend on:</p><ul><li><p>The filesystem the files are in. Many systems like HDFS or S3 don&rsquo;t support append operations, so the only solution is to read each file, concatenate it in-memory and write the output. Obviously if this is done in-memory it might not be possible in Big Data contexts.</p></li><li><p>The file format you are using. Many file formats do not support simply appending to the end of a file, so again only in-memory processing with a library that understands the format can be done.</p></li><li><p>The compression format being used. Same as before: you can&rsquo;t simply <code>cat</code> two zip files expecting them to magically merge.</p></li></ul><p>So again, there&rsquo;s no easy, works-for-all solution here.</p><h3 id=the-s3-workaround>The S3 workaround</h3><p>If you are dealing with plain-text, S3-stored datsets, there&rsquo;s some light at the end of the tunnel. S3 allows for <a href=https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html>multipart upload operations</a>, which let users upload/copy large files at faster speeds by (like Spark does) dividing them into smaller chunks and submitting them individually.</p><p>Multipart uploads can be used to copy, and in this case one could do one-to-many or many-to-one operations - the latter being exactly what we want here. AWS demonstrates such a scenario with the Ruby SDK <a href=https://aws.amazon.com/blogs/developer/efficient-amazon-s3-object-concatenation-using-the-aws-sdk-for-ruby/>here</a>, but it could be implemented in any language, and it could be sped up with threading since <a href=https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html#distributedmpupload>multipart uploads can be concurrent</a>. A simple test on my end reveals ~10 seconds to compact 1GB worth of JSON files - which is actually pretty good.</p><h2 id=conclusion>Conclusion</h2><p>This is a complex problem that has more to do with industry standards than with any application in particular - data processing and visualization tools have to start accepting <em>a path</em> rather than <em>a file</em> as the input.</p><p>If you are using S3, writing your own custom process to compact uncompressed, plain-text files is possible. If you are running your data processing workloads using AWS Glue for instance, writing a simple Python Shell job that runs after your main Spark job and compacts all files should be quite simple and effective. Even more so, now that <a href=https://aws.amazon.com/about-aws/whats-new/2020/12/aws-glue-launches-aws-glue-custom-connectors/>Glue supports custom connectors</a> one could create a custom connector that writes all data to a single S3 object - although I imagine there&rsquo;s several limitations here such as the S3 transaction limit, and the fact that S3 doesn&rsquo;t really favor submitting large volumes of requests towards the same key. Also there&rsquo;s a 5MB minimum size for multipart parts - which means jobs will fail if partitions are smaller than that.</p><p>In any case this is not something easy to solve, and there&rsquo;s definitely room for a cool project here.</p></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=/posts>Writings</a></li><li><a href=/tags>Tags</a></li><li><a href=/about>About</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#why-does-this-happen>Why does this happen?</a></li><li><a href=#how-do-i-fix-this>How do I fix this?</a><ul><li><a href=#repartitioning-to-1-partition>Repartitioning to 1 partition</a></li><li><a href=#compacting-after-writing>Compacting after writing</a></li><li><a href=#the-s3-workaround>The S3 workaround</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div><div id=share-footer style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f"><i class="fab fa-facebook fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f&text=The%20%22Spark%20writes%20too%20many%20output%20files%22%20conundrum"><i class="fab fa-twitter fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f&title=The%20%22Spark%20writes%20too%20many%20output%20files%22%20conundrum"><i class="fab fa-linkedin fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f&is_video=false&description=The%20%22Spark%20writes%20too%20many%20output%20files%22%20conundrum"><i class="fab fa-pinterest fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=The%20%22Spark%20writes%20too%20many%20output%20files%22%20conundrum&body=Check out this article: https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f"><i class="fas fa-envelope fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f&title=The%20%22Spark%20writes%20too%20many%20output%20files%22%20conundrum"><i class="fab fa-get-pocket fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f&title=The%20%22Spark%20writes%20too%20many%20output%20files%22%20conundrum"><i class="fab fa-reddit fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f&name=The%20%22Spark%20writes%20too%20many%20output%20files%22%20conundrum&description=As%20a%20Big%20Data%20engineer%2c%20one%20of%20the%20most%20common%20problems%20I%20see%20people%20facing%20is%20%26ldquo%3bwhy%20is%20Spark%20writing%20300%20files%26rdquo%3b.%20Many%20engineers%20%28specially%20ones%20in%20BI%20environments%29%20are%20used%20to%20having%20datasets%20written%20as%20a%20single%20file%2c%20typically%20in%20a%20plain%20text%20format%20like%20CSV%20or%20JSON.%20Not%20only%20that%2c%20but%20many%20tools%20like%20Databases%20or%20BI%20dashboards%20expect%20data%20to%20be%20part%20of%20a%20single%20file%20-%20which%20obviously%20clases%20with%20how%20Spark%20works."><i class="fab fa-tumblr fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2falbertquiroga.github.io%2fposts%2fspark-output-files-conundrum%2f&t=The%20%22Spark%20writes%20too%20many%20output%20files%22%20conundrum"><i class="fab fa-hacker-news fa-lg" aria-hidden=true></i></a></li></ul></div><div id=actions-footer><a id=menu class=icon href=# onclick="return $('#nav-footer').toggle(),!1"><i class="fas fa-bars fa-lg" aria-hidden=true></i> Menu</a>
<a id=toc class=icon href=# onclick="return $('#toc-footer').toggle(),!1"><i class="fas fa-list fa-lg" aria-hidden=true></i> TOC</a>
<a id=share class=icon href=# onclick="return $('#share-footer').toggle(),!1"><i class="fas fa-share-alt fa-lg" aria-hidden=true></i> share</a>
<a id=top style=display:none class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast')"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i> Top</a></div></div></div><footer id=footer><div class=footer-left>Copyright &copy; 2021 Albert Quiroga</div><div class=footer-right><nav><ul><li><a href=/>Home</a></li><li><a href=/posts>Writings</a></li><li><a href=/tags>Tags</a></li><li><a href=/about>About</a></li></ul></nav></div></footer></div></body><link rel=stylesheet href=/lib/font-awesome/css/all.min.css><script src=/lib/jquery/jquery.min.js></script><script src=/js/main.js></script></html>